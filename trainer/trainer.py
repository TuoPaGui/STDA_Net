# å¯¼å…¥æ¨¡å‹
# å¯¼å…¥æ•°æ®ç”Ÿæˆå·¥å…·
from trainer_tools import data_labels_generate
from trainer_tools import calculate_steps
from trainer_tools import trainer_log
from trainer_tools import plot_metrics
from trainer_tools import metrics
from trainer_tools import early_stopping
from trainer_tools import get_batch_class_weights
# å­¦ä¹ æ¡†æ¶
import torch
import torch.optim as optim # ä¼˜åŒ–å™¨
from torch.optim import lr_scheduler # è°ƒåº¦å™¨
# ===== æ•°æ®ä¸ç³»ç»Ÿå¤„ç†åº“ =====
import glob                                        # æ–‡ä»¶åŒ¹é…æ¨¡å—ï¼Œç”¨äºè·å– .h5 æ•°æ®æ–‡ä»¶åˆ—è¡¨
import random                                      # Python éšæœºæ•°å·¥å…·ï¼ˆç”¨äºæ‰“ä¹±æ•°æ®ï¼‰
from sklearn.model_selection import KFold         # KæŠ˜äº¤å‰éªŒè¯åˆ’åˆ†å™¨
import os                                          # ç³»ç»Ÿæ“ä½œæ¨¡å—ï¼ˆè·¯å¾„ã€æ–‡ä»¶è¯»å†™ï¼‰
import numpy as np                                 # æ•°å€¼è®¡ç®—åº“ï¼Œä¸»è¦ç”¨äºæ ‡ç­¾ã€æƒé‡å¤„ç†
import torch.nn as nn                              # ç¥ç»ç½‘ç»œæ¨¡å—ï¼ˆå¦‚ Linearã€Lossã€Convï¼‰
import json                                        # è¯»å–é…ç½®æ–‡ä»¶ï¼ˆconfig.jsonï¼‰
import importlib
from tqdm import tqdm  # å¯¼å…¥tqdmæ¨¡å—ï¼Œç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡


def log_message(message):
    log_file_path = os.path.join("G:\\Research\\EEG_Project\\Template\\CodeDir", 'æ¨¡å‹è®­ç»ƒæ—¥å¿—.txt')
    with open(log_file_path, "a", encoding="utf-8") as log_file:
        log_file.write(f"{message}\n")

# åŠ è½½é…ç½®æ–‡ä»¶
with open("G:/Research/EEG_Project/Template/CodeDir/config/sda_net_config.json", 'r') as f:
    config = json.load(f)

class Trainer:
    def __init__(self):

        # ========= è¯»å–é…ç½® =========

        # è®­ç»ƒå‚æ•°
        self.trainer_cfg = config["trainer"]
        self.seed = self.trainer_cfg["seed"]
        self.seed2 = self.trainer_cfg["seed2"]
        self.num_epochs = self.trainer_cfg["num_epochs"]
        self.k_folds = self.trainer_cfg["k_folds"]
        self.batches = self.trainer_cfg["batch_size"]
        self.patience = self.trainer_cfg["patience"]

        # è·¯å¾„
        self.path_cfg = config["paths"]
        self.data_files_path = self.path_cfg["data_dir"]
        self.model_path = self.path_cfg["model_path"]
        self.log_dir = self.path_cfg["log_dir"]
        self.csv_output_dir = self.path_cfg["csv_output_dir"]

        # ========= æ„å»ºæ¨¡å‹ä¸ç»„ä»¶ =========
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model_cfg = config["model"]
        self.model_args = model_cfg["args"]
        module = importlib.import_module(f"model.{model_cfg['module']}")
        self.ModelClass = getattr(module, model_cfg["type"])
        self.model = self.ModelClass(**self.model_args).to(self.device)

        # ä¼˜åŒ–å™¨
        optimizer_type = config["optimizer"]["type"]
        self.opt_args = config["optimizer"]["args"]
        self.OptimizerClass = getattr(optim, optimizer_type)
        self.optimizer = self.OptimizerClass(self.model.parameters(), **self.opt_args)

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_type = config["scheduler"]["type"]
        self.sched_args = config["scheduler"]["args"]
        self.SchedulerClass = getattr(lr_scheduler, scheduler_type)
        self.scheduler = self.SchedulerClass(self.optimizer, **self.sched_args)

        # æŸå¤±å‡½æ•°
        loss_type = config["loss"]["type"]
        self.loss_args = config["loss"]["args"]
        self.LossClass = getattr(nn, loss_type)
        self.criterion = self.LossClass(**self.loss_args)

        # ç›¸å…³å˜é‡å®šä¹‰
        self.k_fold_splitter = None
        self.epoch = 0
        self.fold = 0
        self.train_files = []
        self.val_files = []
        self.train_steps = 0
        self.val_steps = 0
        self.best_train_metrics = {'accuracy': -float('inf'), 'f1': -float('inf'), 'kappa': -float('inf')}
        self.best_val_metrics = {'accuracy': -float('inf'), 'f1': -float('inf'), 'kappa': -float('inf')}
        self.num_classes = 5
        self.start_epoch = 0
        self.last_weights = None    # è®¡ç®—ç±»åˆ«æƒé‡ç”¨

        # æ•°æ®è·¯å¾„
        self.data_files = glob.glob(os.path.join(self.data_files_path, "*.h5"))

        # åˆå§‹åŒ–æ•°æ®ç”Ÿæˆå™¨
        self.data_labels_generator = data_labels_generate.DataLabelsGenerate()
        self.train_data_generator = None
        self.val_data_generator = None

        # éªŒè¯æ¨¡å‹æ•ˆæœç”¨
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []

        # æ—©åœç›¸å…³æŒ‡æ ‡
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.no_improve_epochs = 0
        self.early_stopper = early_stopping.EarlyStopping(
            patience=self.patience,
            save_dir=self.model_path,
            logger=log_message
        )

    def data_init(self):

        # Python å’Œ NumPy çš„ç§å­
        random.seed(self.seed)
        np.random.seed(self.seed )

        # PyTorch CPU å’Œ GPU çš„ç§å­
        torch.manual_seed(self.seed2 )
        torch.cuda.manual_seed(self.seed2 )
        torch.cuda.manual_seed_all(self.seed2 )

        random.shuffle(self.data_files)
        self.k_fold_splitter = KFold(n_splits = self.k_folds, shuffle = False)
        # self.k_fold_splitter = KFold(n_splits = self.k_folds, shuffle = True, random_state=int(time.time()))  # åˆ†æˆk_foldsç»„ éšæœºæ‰“ä¹±
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    # æ¯ä¸€æŠ˜è®­ç»ƒå‰çš„åˆå§‹åŒ–
    def fold_init(self,fold):
        self.fold = fold
        # é‡ç½®æ—©åœå‚æ•°
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.no_improve_epochs = 0
        self.best_train_metrics = {'accuracy': -float('inf'), 'f1': -float('inf'), 'kappa': -float('inf')}
        self.best_val_metrics = {'accuracy': -float('inf'), 'f1': -float('inf'), 'kappa': -float('inf')}

        # æ¯æŠ˜è®­ç»ƒå‰é‡ç½®æ¨¡å‹åŠä¼˜åŒ–å™¨
        self.model = self.ModelClass(**self.model_args).to(self.device)

        self.optimizer = self.OptimizerClass(self.model.parameters(), **self.opt_args)
        self.scheduler = self.SchedulerClass(self.optimizer, **self.sched_args)
        self.criterion = self.LossClass(**self.loss_args)

        self.early_stopper = early_stopping.EarlyStopping(patience=self.patience,save_dir=self.model_path,logger=log_message)
        print(f"Training fold {fold + 1}/{self.k_folds}...")
        log_message(f"Training fold {fold + 1}/{self.k_folds}...")


    def load_checkpoint(self, fold):
        model_path = os.path.join(self.model_path, f"model_fold_{self.fold + 1}.pth")
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            self.start_epoch = checkpoint.get('epoch', 0) + 1
            self.best_val_acc = checkpoint.get('best_val_accuracy', 0.0)
            self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
            self.train_losses = checkpoint.get('train_losses', [])
            self.val_losses = checkpoint.get('val_losses', [])
            self.train_accuracies = checkpoint.get('train_accuracies', [])
            self.val_accuracies = checkpoint.get('val_accuracies', [])
            print(f"âœ… Loaded checkpoint from {model_path}, resuming from epoch {self.start_epoch}")
            return True
        except Exception as e:
            print(f"âŒ Failed to load checkpoint for fold {fold + 1}: {e}")
            self.start_epoch = 0
            return False

    def train_per_epoch(self):
        epoch_loss = 0.0
        train_preds, train_labels = [], []
        self.model.train()
        progress_bar = tqdm(range(self.train_steps), desc=f"Epoch {self.epoch + 1}", ncols=100)

        for _ in progress_bar:
            batch = next(self.train_data_generator)
            data, labels = batch['batch_data'].to(self.device), batch['labels'].to(self.device)

            # è®¡ç®—åŠ¨æ€ç±»æƒé‡
            batch_weights = get_batch_class_weights.get_batch_class_weights(labels, num_classes=self.num_classes, device=self.device, prev_weights=self.last_weights)
            self.last_weights = batch_weights.detach()
            loss_args = self.loss_args.copy()
            loss_args["weight"] = batch_weights
            # æ›´æ–°ç±»åˆ«æƒé‡ æ³¨é‡Šæ‰åˆ™æ˜¯ç”¨é»˜è®¤mean
            self.criterion = self.LossClass(**loss_args)

            self.optimizer.zero_grad()  # æ¸…é™¤ä»¥å‰çš„æ¢¯åº¦
            outputs = self.model(data, labels, self.criterion)

            loss = outputs['total_loss']
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)  # æ–°å¢æ¢¯åº¦è£å‰ª
            self.optimizer.step()  # æ›´æ–°å‚æ•° æ›´æ–°æƒé‡å’Œåç½® æ¯”å¦‚å·ç§¯ã€å…¨è¿æ¥ä¸­çš„æ¯ä¸€å±‚ éƒ½æœ‰ç›¸åº”çš„æƒé‡å’Œåç½®çŸ©é˜µ
            epoch_loss += loss.item()

            train_preds.extend(torch.argmax(outputs['logits'], 1).cpu().numpy())
            train_labels.extend(labels.cpu().numpy())

            progress_bar.set_postfix({'Loss': loss.item()})

        return epoch_loss / self.train_steps, train_preds, train_labels

    def validate_per_epoch(self):
        self.model.eval()
        val_loss = 0.0
        all_preds, all_labels = [], []

        with torch.no_grad():
            progress_bar = tqdm(range(self.val_steps), desc=f"Validation Epoch {self.epoch + 1}", ncols=100)
            for _ in progress_bar:
                batch = next(self.val_data_generator)
                data, labels = batch['batch_data'].to(self.device), batch['labels'].to(self.device)
                outputs = self.model(data, labels, self.criterion)
                loss = outputs['total_loss']
                val_loss += loss.item()

                preds = torch.argmax(outputs['logits'], 1)

                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                progress_bar.set_postfix({'Val Loss': loss.item()})

                # æ‰¾å‡ºæ•ˆæœè¾ƒå·®çš„æ–‡ä»¶
                if self.epoch > 16:
                    if loss > 0.7:
                        log_message(f"Bad file: {batch['file_name']} Loss: {loss}")

        print(f"å®é™…å€¼:{all_labels}")
        print(f"é¢„æµ‹å€¼:{all_preds}")
        return val_loss / self.val_steps, all_preds, all_labels

    def model_train_epochs(self):
        # æ•°æ®åˆå§‹åŒ–
        self.data_init()

        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸Šæ¬¡æ–­ç‚¹æ–‡ä»¶
        resume_fold = 0
        if os.path.exists("resume_state.json"):
            with open("resume_state.json", "r") as f:
                resume_fold = int(f.read().strip())
            print(f"ğŸ” Resume from fold {resume_fold + 1}")
        else:
            print("ğŸš€ Starting from fold 1")

        # é‡‡ç”¨KæŠ˜å éªŒè¯   å› æ­¤é€‰ç”¨æ‰€æœ‰æ–‡ä»¶
        # train_val_files, test_files = train_test_split(self.data_files, test_size=0.1, random_state=42)
        train_val_files = self.data_files

        for fold, (train_index, val_index)  in enumerate(self.k_fold_splitter.split(train_val_files)):
            if fold < resume_fold:
                continue  # è·³è¿‡å·²å®Œæˆçš„æŠ˜æ•°
            # æŠ˜è®­ç»ƒå‰çš„åˆå§‹åŒ–
            self.fold_init(fold)

            self.train_files = [train_val_files[i] for i in train_index]
            self.val_files = [train_val_files[i] for i in val_index]

            self.train_steps = calculate_steps.calculate_steps_per_epoch(self.train_files,self.batches)
            self.val_steps = calculate_steps.calculate_steps_per_epoch(self.val_files,self.batches)

            log_message(f"train_files: {self.train_files}")
            log_message(f"val_files: {self.val_files}")

            self.train_data_generator = self.data_labels_generator.generate_data_labels(self.train_files, self.batches)
            self.val_data_generator = self.data_labels_generator.generate_data_labels(self.val_files, self.batches)

            # åŠ è½½æ–­ç‚¹
            self.load_checkpoint(fold)

            for self.epoch in range(self.start_epoch, self.num_epochs):
                # è®­ç»ƒé˜¶æ®µ
                train_loss, train_preds, train_labels = self.train_per_epoch()
                train_metrics = metrics.calculate_metrics(train_preds, train_labels)

                # éªŒè¯é˜¶æ®µ
                val_loss, val_preds, val_labels = self.validate_per_epoch()
                val_metrics = metrics.calculate_metrics(val_preds, val_labels)

                # æ›´æ–°å­¦ä¹ ç‡
                self.scheduler.step(val_loss)
                current_lr = self.optimizer.param_groups[0]['lr']
                # æ‰“å°æŒ‡æ ‡
                print(f"\nFold {fold + 1} Epoch {self.epoch + 1}/{self.num_epochs}")
                print(f"Current Learning Rate: {current_lr:.6f}")
                print(f"[Train] Loss: {train_loss:.4f} | Acc: {train_metrics['accuracy']:.4f} | F1: {train_metrics['f1']:.4f} | Kappa: {train_metrics['kappa']:.4f}")
                print(f"[ Val ] Loss: {val_loss:.4f} | Acc: {val_metrics['accuracy']:.4f} | F1: {val_metrics['f1']:.4f} | Kappa: {val_metrics['kappa']:.4f}")

                # ä¿å­˜æœ€ä¼˜æŒ‡æ ‡
                if train_metrics['accuracy'] > self.best_train_metrics['accuracy']:
                    self.best_train_metrics = train_metrics
                if val_metrics['accuracy'] > self.best_val_metrics['accuracy']:
                    self.best_val_metrics = val_metrics

                # è®°å½•æ¯ä¸ªepochçš„è®­ç»ƒä¿¡æ¯
                log_message(f"{'*' * 40} {self.epoch + 1} {'*' * 40}")
                log_message("Evaluation Results:")
                log_message(f"Val_Accuracy: {val_metrics['accuracy']:.4f} | F1: {val_metrics['f1']:.4f} | Kappa: {val_metrics['kappa']:.4f}")
                class_names = ["W", "N1", "N2", "N3", "REM"]
                trainer_log.log_confusion_matrix_with_metrics(train_metrics['confusion_matrix_percent'],train_metrics['precision'], train_metrics['f1_score'],train_metrics['recall'], class_names, title="Train")
                trainer_log.log_confusion_matrix_with_metrics(val_metrics['confusion_matrix_percent'], val_metrics['precision'],val_metrics['f1_score'], val_metrics['recall'], class_names,title="Val")
                log_message(f"Train_Loss: {train_loss:.4f} | ")
                log_message(f"Val_Loss: {val_loss:.4f} | ")
                log_message(f"Train_Accuracy: {train_metrics['accuracy']:.4f} | ")
                log_message(f"Val_Accuracy: {val_metrics['accuracy']:.4f} | ")
                log_message(f"Best_train_accuracy : {self.best_train_metrics['accuracy']:.4f} |")
                log_message(f"Best_val_accuracy : {self.best_val_metrics['accuracy']:.4f} |")

                # æ›´æ–°æŒ‡æ ‡å­˜å‚¨
                self.train_losses.append(train_loss)
                self.val_losses.append(val_loss)
                self.train_accuracies.append(train_metrics['accuracy'])
                self.val_accuracies.append(val_metrics['accuracy'])
                # å®æ—¶æ›´æ–°æ›²çº¿å›¾
                plot_metrics.plot_metrics(self.train_losses,self.val_losses,self.train_accuracies,self.val_accuracies)

                # æ—©åœæ£€æŸ¥
                should_stop = self.early_stopper.step(
                    fold=fold,
                    epoch=self.epoch,
                    val_loss=val_loss,
                    val_acc=val_metrics['accuracy'],
                    model=self.model,
                    optimizer=self.optimizer,
                    scheduler=self.scheduler,
                    train_losses=self.train_losses,
                    val_losses=self.val_losses,
                    train_accuracies=self.train_accuracies,
                    val_accuracies=self.val_accuracies
                )
                if should_stop:
                    with open("resume_state.json", "w") as file:
                        file.write(str(fold))
                    break


        log_message(f"[Best Train] Acc: {self.best_train_metrics['accuracy']:.4f} | F1: {self.best_train_metrics['f1']:.4f} | Kappa: {self.best_train_metrics['kappa']:.4f}")
        log_message(f"[ Best Val ] Acc: {self.best_val_metrics['accuracy']:.4f} | F1: {self.best_val_metrics['f1']:.4f} | Kappa: {self.best_val_metrics['kappa']:.4f}")
        log_message("\n Training completed!")
        # ç§»é™¤æŠ˜å è®­ç»ƒè®°å½•æ–‡ä»¶
        if os.path.exists("resume_state.json"):
            os.remove("resume_state.json")
            print("ğŸ§¹ Training complete, removed resume_state.json")

